{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "#test if gpu is wokring\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print(tf.test.gpu_device_name())\n",
    "\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D:\\\\User_Data\\\\Desktop\\\\kan-2\\\\NeuralNetwork\\\\\"\n",
    "import os\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary package\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                2544      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 200       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "=================================================================\n",
      "Total params: 2,816\n",
      "Trainable params: 2,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model complie\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=105, kernel_initializer='normal', activation='relu'))#input_dim=number of columns original :relu\n",
    "model.add(Dense(8, activation='relu'))\n",
    "#model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))#Here the source is using linear \n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_stata(\"CD_icd.dta\")\n",
    "df=pd.read_stata(\"CD_icd.dta\",convert_missing=False)\n",
    "df=df.dropna()\n",
    "x_vars=[\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"month1\",\"month2\",\"month3\",\"month4\",\"month5\",\"month6\"\n",
    "  ,\"month7\",\"month8\",\"month9\",\"month10\",\"month11\",\"month12\",\"male\",\"age\",\"icd_0diab2_365\",\"icd_0liver_365\",\"icd_0cardio_365\",\"icd_0hbp_365\"]\n",
    "for i in df.columns:\n",
    "    if \"icd\" in i and i not in x_vars:\n",
    "        x_vars.append(i)\n",
    "x=df[x_vars]\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "#poly = PolynomialFeatures(degree=2,interaction_only=True,include_bias = False)\n",
    "#x=poly.fit_transform(x)\n",
    "y=df[[\"drug_amt\"]]\n",
    "#icd_auri1\n",
    "scaler_x = MinMaxScaler()#able to change to other \n",
    "scaler_y = MinMaxScaler()#able to change\n",
    "#print(scaler_x.fit(x))\n",
    "#xscale=scaler_x.transform(x)\n",
    "#print(scaler_y.fit(y))\n",
    "#yscale=scaler_y.transform(y)\n",
    "xscale=x\n",
    "yscale=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_step=15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15000\n",
      "12943/12943 [==============================] - 10s 803us/step - loss: 1463751.7500 - mse: 1463751.8750 - mae: 346.1436 - val_loss: 1012742.2500 - val_mse: 1012742.2500 - val_mae: 336.3873\n",
      "Epoch 2/15000\n",
      "12943/12943 [==============================] - 10s 800us/step - loss: 1442567.0000 - mse: 1442567.0000 - mae: 332.8363 - val_loss: 1010214.8125 - val_mse: 1010214.8125 - val_mae: 332.9013\n",
      "Epoch 3/15000\n",
      "12943/12943 [==============================] - 11s 822us/step - loss: 1440178.5000 - mse: 1440178.5000 - mae: 332.0423 - val_loss: 1012455.7500 - val_mse: 1012455.7500 - val_mae: 351.7679\n",
      "Epoch 4/15000\n",
      "12943/12943 [==============================] - 10s 807us/step - loss: 1438823.2500 - mse: 1438823.2500 - mae: 332.1355 - val_loss: 1009810.4375 - val_mse: 1009810.4375 - val_mae: 340.5180\n",
      "Epoch 5/15000\n",
      "12943/12943 [==============================] - 10s 805us/step - loss: 1437640.1250 - mse: 1437640.1250 - mae: 332.1843 - val_loss: 1008196.8750 - val_mse: 1008196.8750 - val_mae: 326.1638\n",
      "Epoch 6/15000\n",
      "12943/12943 [==============================] - 10s 798us/step - loss: 1436075.5000 - mse: 1436075.5000 - mae: 332.1790 - val_loss: 1008539.0625 - val_mse: 1008539.0625 - val_mae: 331.3363\n",
      "Epoch 7/15000\n",
      "12943/12943 [==============================] - 10s 806us/step - loss: 1434226.3750 - mse: 1434226.3750 - mae: 332.0810 - val_loss: 1008315.3750 - val_mse: 1008315.3750 - val_mae: 328.5539\n",
      "Epoch 8/15000\n",
      "12943/12943 [==============================] - 11s 818us/step - loss: 1431858.2500 - mse: 1431858.2500 - mae: 331.6385 - val_loss: 1010744.6875 - val_mse: 1010744.6875 - val_mae: 333.5190\n",
      "Epoch 9/15000\n",
      "12943/12943 [==============================] - 11s 815us/step - loss: 1429476.3750 - mse: 1429476.3750 - mae: 331.4487 - val_loss: 1010786.5000 - val_mse: 1010786.5000 - val_mae: 332.6496\n",
      "Epoch 10/15000\n",
      "12943/12943 [==============================] - 11s 820us/step - loss: 1427031.8750 - mse: 1427031.8750 - mae: 331.2593 - val_loss: 1015628.8750 - val_mse: 1015628.8750 - val_mae: 346.6903\n",
      "Epoch 11/15000\n",
      "12943/12943 [==============================] - 11s 830us/step - loss: 1424765.5000 - mse: 1424765.5000 - mae: 331.2340 - val_loss: 1012067.7500 - val_mse: 1012067.7500 - val_mae: 330.6838\n",
      "Epoch 12/15000\n",
      "12943/12943 [==============================] - 11s 821us/step - loss: 1422438.5000 - mse: 1422438.5000 - mae: 331.0349 - val_loss: 1011151.4375 - val_mse: 1011151.4375 - val_mae: 326.3542\n",
      "Epoch 13/15000\n",
      "12943/12943 [==============================] - 11s 818us/step - loss: 1419982.5000 - mse: 1419982.5000 - mae: 330.7625 - val_loss: 1015967.6250 - val_mse: 1015967.6250 - val_mae: 327.9017\n",
      "Epoch 14/15000\n",
      "12943/12943 [==============================] - 11s 829us/step - loss: 1417712.0000 - mse: 1417712.0000 - mae: 330.5390 - val_loss: 1014346.7500 - val_mse: 1014346.7500 - val_mae: 330.3266\n",
      "Epoch 15/15000\n",
      "12943/12943 [==============================] - 11s 821us/step - loss: 1414798.7500 - mse: 1414798.7500 - mae: 330.1781 - val_loss: 1014646.0000 - val_mse: 1014646.0000 - val_mae: 328.9211\n",
      "Epoch 16/15000\n",
      "12943/12943 [==============================] - 11s 850us/step - loss: 1412455.3750 - mse: 1412455.3750 - mae: 329.7760 - val_loss: 1016712.8750 - val_mse: 1016712.8750 - val_mae: 331.7817\n",
      "Epoch 17/15000\n",
      "12943/12943 [==============================] - 11s 816us/step - loss: 1391428.1250 - mse: 1391428.1250 - mae: 329.0225 - val_loss: 991896.3125 - val_mse: 991896.3125 - val_mae: 326.5462\n",
      "Epoch 18/15000\n",
      "12943/12943 [==============================] - 11s 838us/step - loss: 1379462.6250 - mse: 1379462.6250 - mae: 329.2081 - val_loss: 997703.0000 - val_mse: 997703.0000 - val_mae: 336.7287\n",
      "Epoch 19/15000\n",
      "12943/12943 [==============================] - 11s 824us/step - loss: 1376710.7500 - mse: 1376710.7500 - mae: 329.3822 - val_loss: 994172.1250 - val_mse: 994172.1250 - val_mae: 326.1904\n",
      "Epoch 20/15000\n",
      "12943/12943 [==============================] - 11s 814us/step - loss: 1375382.2500 - mse: 1375382.2500 - mae: 329.1047 - val_loss: 995891.7500 - val_mse: 995891.7500 - val_mae: 332.9239\n",
      "Epoch 21/15000\n",
      "12943/12943 [==============================] - 11s 814us/step - loss: 1373322.1250 - mse: 1373322.1250 - mae: 328.8533 - val_loss: 995136.8125 - val_mse: 995136.8125 - val_mae: 321.3870\n",
      "Epoch 22/15000\n",
      "12943/12943 [==============================] - 11s 828us/step - loss: 1371796.8750 - mse: 1371796.8750 - mae: 328.4612 - val_loss: 998185.2500 - val_mse: 998185.2500 - val_mae: 341.8613\n",
      "Epoch 23/15000\n",
      "12943/12943 [==============================] - 11s 833us/step - loss: 1370125.2500 - mse: 1370125.2500 - mae: 328.5490 - val_loss: 999865.0625 - val_mse: 999865.0625 - val_mae: 349.2815\n",
      "Epoch 24/15000\n",
      "12943/12943 [==============================] - 10s 804us/step - loss: 1369169.5000 - mse: 1369169.5000 - mae: 328.6557 - val_loss: 996325.7500 - val_mse: 996325.7500 - val_mae: 323.3612\n",
      "Epoch 25/15000\n",
      "12943/12943 [==============================] - 11s 886us/step - loss: 1367732.6250 - mse: 1367732.6250 - mae: 328.2617 - val_loss: 1001394.8125 - val_mse: 1001394.8125 - val_mae: 343.6952\n",
      "Epoch 26/15000\n",
      "12943/12943 [==============================] - 11s 835us/step - loss: 1366176.1250 - mse: 1366176.1250 - mae: 328.1378 - val_loss: 995036.4375 - val_mse: 995036.4375 - val_mae: 322.8157\n",
      "Epoch 27/15000\n",
      "12943/12943 [==============================] - 10s 804us/step - loss: 1365316.7500 - mse: 1365316.7500 - mae: 328.1903 - val_loss: 1002664.1875 - val_mse: 1002664.1875 - val_mae: 320.8008\n",
      "Epoch 28/15000\n",
      "12943/12943 [==============================] - 11s 813us/step - loss: 1363910.3750 - mse: 1363910.3750 - mae: 327.3806 - val_loss: 1012904.3750 - val_mse: 1012904.3750 - val_mae: 364.2074\n",
      "Epoch 29/15000\n",
      "12943/12943 [==============================] - 11s 817us/step - loss: 1362333.1250 - mse: 1362333.1250 - mae: 327.5230 - val_loss: 995454.8125 - val_mse: 995454.8125 - val_mae: 322.7327\n",
      "Epoch 30/15000\n",
      "12943/12943 [==============================] - 11s 813us/step - loss: 1361890.3750 - mse: 1361890.3750 - mae: 327.2143 - val_loss: 1001216.0000 - val_mse: 1001216.0000 - val_mae: 325.8512\n",
      "Epoch 31/15000\n",
      "12943/12943 [==============================] - 10s 809us/step - loss: 1360592.1250 - mse: 1360592.1250 - mae: 326.9477 - val_loss: 1000320.5625 - val_mse: 1000320.5625 - val_mae: 316.6581\n",
      "Epoch 32/15000\n",
      "12943/12943 [==============================] - 10s 801us/step - loss: 1359794.5000 - mse: 1359794.5000 - mae: 326.6034 - val_loss: 1003234.5000 - val_mse: 1003234.5000 - val_mae: 338.3858\n",
      "Epoch 33/15000\n",
      "12943/12943 [==============================] - 11s 824us/step - loss: 1357876.6250 - mse: 1357876.6250 - mae: 326.7711 - val_loss: 1006018.2500 - val_mse: 1006018.2500 - val_mae: 333.2308\n",
      "Epoch 34/15000\n",
      "12943/12943 [==============================] - 11s 819us/step - loss: 1356600.5000 - mse: 1356600.5000 - mae: 326.5019 - val_loss: 1002070.1875 - val_mse: 1002070.1875 - val_mae: 329.3669\n",
      "Epoch 35/15000\n",
      "12943/12943 [==============================] - 11s 822us/step - loss: 1355171.3750 - mse: 1355171.3750 - mae: 326.2724 - val_loss: 1014333.6875 - val_mse: 1014333.6875 - val_mae: 351.3088\n",
      "Epoch 36/15000\n",
      "12943/12943 [==============================] - 11s 817us/step - loss: 1354048.6250 - mse: 1354048.6250 - mae: 326.3004 - val_loss: 1004740.6250 - val_mse: 1004740.6250 - val_mae: 329.9141\n",
      "Epoch 37/15000\n",
      "12943/12943 [==============================] - 11s 815us/step - loss: 1352560.5000 - mse: 1352560.5000 - mae: 326.3292 - val_loss: 1002577.7500 - val_mse: 1002578.0000 - val_mae: 328.3910\n",
      "Epoch 38/15000\n",
      "12943/12943 [==============================] - 10s 804us/step - loss: 1350980.5000 - mse: 1350980.2500 - mae: 326.0759 - val_loss: 1002870.9375 - val_mse: 1002870.9375 - val_mae: 325.9902\n",
      "Epoch 39/15000\n",
      "12943/12943 [==============================] - 11s 819us/step - loss: 1350000.0000 - mse: 1350000.0000 - mae: 326.0819 - val_loss: 1004998.1250 - val_mse: 1004998.1250 - val_mae: 337.0245\n",
      "Epoch 40/15000\n",
      "12943/12943 [==============================] - 11s 819us/step - loss: 1349204.2500 - mse: 1349204.1250 - mae: 326.3216 - val_loss: 1004878.7500 - val_mse: 1004878.7500 - val_mae: 322.0536\n",
      "Epoch 41/15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12943/12943 [==============================] - 10s 788us/step - loss: 1347150.1250 - mse: 1347150.1250 - mae: 326.2096 - val_loss: 1000896.2500 - val_mse: 1000896.2500 - val_mae: 328.2551\n",
      "Epoch 42/15000\n",
      "12943/12943 [==============================] - 10s 783us/step - loss: 1346735.3750 - mse: 1346735.3750 - mae: 325.9359 - val_loss: 1008495.1250 - val_mse: 1008495.1250 - val_mae: 335.0264\n",
      "Epoch 43/15000\n",
      "12943/12943 [==============================] - 10s 791us/step - loss: 1345280.5000 - mse: 1345280.3750 - mae: 326.2546 - val_loss: 1008128.5625 - val_mse: 1008128.5625 - val_mae: 319.7172\n",
      "Epoch 44/15000\n",
      "12943/12943 [==============================] - 10s 796us/step - loss: 1343317.2500 - mse: 1343317.2500 - mae: 326.0228 - val_loss: 1011446.6875 - val_mse: 1011446.6875 - val_mae: 346.2147\n",
      "Epoch 45/15000\n",
      "12943/12943 [==============================] - 11s 879us/step - loss: 1343700.8750 - mse: 1343700.8750 - mae: 325.8565 - val_loss: 1013549.8750 - val_mse: 1013549.8750 - val_mae: 354.6642\n",
      "Epoch 46/15000\n",
      "12943/12943 [==============================] - 10s 791us/step - loss: 1342348.5000 - mse: 1342348.5000 - mae: 326.1348 - val_loss: 1014384.4375 - val_mse: 1014384.4375 - val_mae: 336.9273\n",
      "Epoch 47/15000\n",
      "12943/12943 [==============================] - 10s 781us/step - loss: 1342092.8750 - mse: 1342092.8750 - mae: 326.2956 - val_loss: 1019759.6875 - val_mse: 1019759.6875 - val_mae: 347.3703\n",
      "Epoch 48/15000\n",
      "12943/12943 [==============================] - 10s 785us/step - loss: 1340286.1250 - mse: 1340286.1250 - mae: 326.0460 - val_loss: 1012210.4375 - val_mse: 1012210.4375 - val_mae: 322.3677\n",
      "Epoch 49/15000\n",
      "12943/12943 [==============================] - 10s 783us/step - loss: 1339815.3750 - mse: 1339815.3750 - mae: 326.1269 - val_loss: 1006837.2500 - val_mse: 1006837.2500 - val_mae: 333.2308\n",
      "Epoch 50/15000\n",
      "12943/12943 [==============================] - 10s 796us/step - loss: 1338783.6250 - mse: 1338783.6250 - mae: 326.1339 - val_loss: 1025802.9375 - val_mse: 1025802.9375 - val_mae: 343.3851\n",
      "Epoch 51/15000\n",
      "12943/12943 [==============================] - 10s 798us/step - loss: 1333446.0000 - mse: 1333446.0000 - mae: 325.4023 - val_loss: 1001843.1875 - val_mse: 1001843.1875 - val_mae: 339.1002\n",
      "Epoch 52/15000\n",
      "12943/12943 [==============================] - 10s 789us/step - loss: 1309226.7500 - mse: 1309226.7500 - mae: 325.1039 - val_loss: 991407.6250 - val_mse: 991407.6250 - val_mae: 333.8251\n",
      "Epoch 53/15000\n",
      "12943/12943 [==============================] - 10s 802us/step - loss: 1305281.8750 - mse: 1305281.8750 - mae: 325.6840 - val_loss: 998934.7500 - val_mse: 998934.7500 - val_mae: 332.8462\n",
      "Epoch 54/15000\n",
      "12943/12943 [==============================] - 10s 799us/step - loss: 1302785.2500 - mse: 1302785.2500 - mae: 325.8203 - val_loss: 1015949.3750 - val_mse: 1015949.3750 - val_mae: 367.6418\n",
      "Epoch 55/15000\n",
      "12943/12943 [==============================] - 10s 785us/step - loss: 1299544.6250 - mse: 1299544.6250 - mae: 325.4157 - val_loss: 987156.6875 - val_mse: 987156.6875 - val_mae: 322.5311\n",
      "Epoch 56/15000\n",
      "12943/12943 [==============================] - 10s 793us/step - loss: 1298683.6250 - mse: 1298683.6250 - mae: 325.3071 - val_loss: 992159.2500 - val_mse: 992159.2500 - val_mae: 340.3063\n",
      "Epoch 57/15000\n",
      "12943/12943 [==============================] - 10s 795us/step - loss: 1297981.2500 - mse: 1297981.2500 - mae: 325.2108 - val_loss: 993425.6250 - val_mse: 993425.6250 - val_mae: 321.1614\n",
      "Epoch 58/15000\n",
      "12943/12943 [==============================] - 10s 786us/step - loss: 1296380.5000 - mse: 1296380.6250 - mae: 324.8908 - val_loss: 1016666.3750 - val_mse: 1016666.3750 - val_mae: 352.6811\n",
      "Epoch 59/15000\n",
      "12943/12943 [==============================] - 10s 786us/step - loss: 1296397.3750 - mse: 1296397.3750 - mae: 325.2242 - val_loss: 1005091.7500 - val_mse: 1005091.7500 - val_mae: 349.8860\n",
      "Epoch 60/15000\n",
      "12943/12943 [==============================] - 10s 778us/step - loss: 1294904.8750 - mse: 1294904.8750 - mae: 324.9477 - val_loss: 998666.3125 - val_mse: 998666.3125 - val_mae: 341.8401\n",
      "Epoch 61/15000\n",
      "12943/12943 [==============================] - 10s 770us/step - loss: 1293286.0000 - mse: 1293286.0000 - mae: 325.0312 - val_loss: 996474.0000 - val_mse: 996474.0000 - val_mae: 335.9216\n",
      "Epoch 62/15000\n",
      "12943/12943 [==============================] - 10s 797us/step - loss: 1293685.0000 - mse: 1293685.0000 - mae: 324.9369 - val_loss: 997040.8750 - val_mse: 997040.8750 - val_mae: 336.4462\n",
      "Epoch 63/15000\n",
      "12943/12943 [==============================] - 10s 802us/step - loss: 1292476.1250 - mse: 1292476.1250 - mae: 324.8753 - val_loss: 1003258.1250 - val_mse: 1003258.0000 - val_mae: 334.5494\n",
      "Epoch 64/15000\n",
      "12943/12943 [==============================] - 10s 778us/step - loss: 1291434.6250 - mse: 1291434.6250 - mae: 324.6262 - val_loss: 1000513.3750 - val_mse: 1000513.3750 - val_mae: 323.9494\n",
      "Epoch 65/15000\n",
      "12943/12943 [==============================] - 10s 790us/step - loss: 1292016.1250 - mse: 1292016.1250 - mae: 324.5651 - val_loss: 1005191.7500 - val_mse: 1005191.7500 - val_mae: 328.1546\n",
      "Epoch 66/15000\n",
      "12943/12943 [==============================] - 10s 802us/step - loss: 1289862.1250 - mse: 1289862.1250 - mae: 324.6553 - val_loss: 1000038.3125 - val_mse: 1000038.3125 - val_mae: 340.7097\n",
      "Epoch 67/15000\n",
      "12943/12943 [==============================] - 10s 783us/step - loss: 1289602.7500 - mse: 1289602.7500 - mae: 324.1674 - val_loss: 1014781.4375 - val_mse: 1014781.4375 - val_mae: 364.7603\n",
      "Epoch 68/15000\n",
      "12943/12943 [==============================] - 10s 782us/step - loss: 1288854.1250 - mse: 1288854.1250 - mae: 324.3877 - val_loss: 997651.8750 - val_mse: 997651.8750 - val_mae: 323.7383\n",
      "Epoch 69/15000\n",
      "12943/12943 [==============================] - 10s 787us/step - loss: 1288208.0000 - mse: 1288208.1250 - mae: 324.3851 - val_loss: 993707.3750 - val_mse: 993707.3750 - val_mae: 327.7641\n",
      "Epoch 70/15000\n",
      "12943/12943 [==============================] - 10s 786us/step - loss: 1287971.8750 - mse: 1287971.8750 - mae: 324.4119 - val_loss: 1005179.3125 - val_mse: 1005179.3125 - val_mae: 339.8164\n",
      "Epoch 71/15000\n",
      "12943/12943 [==============================] - 10s 794us/step - loss: 1286686.1250 - mse: 1286686.1250 - mae: 324.0648 - val_loss: 1011406.6875 - val_mse: 1011406.6875 - val_mae: 337.1763\n",
      "Epoch 72/15000\n",
      "12943/12943 [==============================] - 10s 792us/step - loss: 1286625.8750 - mse: 1286625.8750 - mae: 324.2296 - val_loss: 1011859.3750 - val_mse: 1011859.3750 - val_mae: 342.8434\n",
      "Epoch 73/15000\n",
      "12943/12943 [==============================] - 10s 779us/step - loss: 1286983.2500 - mse: 1286983.2500 - mae: 323.7934 - val_loss: 1011520.5000 - val_mse: 1011520.5000 - val_mae: 353.3529\n",
      "Epoch 74/15000\n",
      "12943/12943 [==============================] - 10s 788us/step - loss: 1285005.1250 - mse: 1285005.1250 - mae: 324.0259 - val_loss: 1008395.0625 - val_mse: 1008395.0625 - val_mae: 315.6706\n",
      "Epoch 75/15000\n",
      "12943/12943 [==============================] - 10s 788us/step - loss: 1285110.1250 - mse: 1285110.1250 - mae: 323.9583 - val_loss: 1011182.5625 - val_mse: 1011182.5625 - val_mae: 334.5185\n",
      "Epoch 76/15000\n",
      "12943/12943 [==============================] - 10s 789us/step - loss: 1283924.6250 - mse: 1283924.6250 - mae: 323.5402 - val_loss: 1007878.6250 - val_mse: 1007878.6250 - val_mae: 343.5782\n",
      "Epoch 77/15000\n",
      "12943/12943 [==============================] - 10s 782us/step - loss: 1283433.6250 - mse: 1283433.6250 - mae: 323.7254 - val_loss: 1007959.1875 - val_mse: 1007959.1875 - val_mae: 326.9138\n",
      "Epoch 78/15000\n",
      "12943/12943 [==============================] - 10s 801us/step - loss: 1281437.8750 - mse: 1281437.8750 - mae: 323.5430 - val_loss: 1008822.7500 - val_mse: 1008822.7500 - val_mae: 342.4343\n",
      "Epoch 79/15000\n",
      "12943/12943 [==============================] - 10s 780us/step - loss: 1282348.2500 - mse: 1282348.3750 - mae: 323.4002 - val_loss: 1008495.5625 - val_mse: 1008495.5625 - val_mae: 345.4909\n",
      "Epoch 80/15000\n",
      "12943/12943 [==============================] - 10s 775us/step - loss: 1281422.3750 - mse: 1281422.3750 - mae: 323.5853 - val_loss: 1003490.2500 - val_mse: 1003490.2500 - val_mae: 330.8251\n",
      "Epoch 81/15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12943/12943 [==============================] - 10s 799us/step - loss: 1280067.2500 - mse: 1280067.2500 - mae: 323.2529 - val_loss: 1004151.7500 - val_mse: 1004151.7500 - val_mae: 333.9738\n",
      "Epoch 82/15000\n",
      "12943/12943 [==============================] - 10s 796us/step - loss: 1280134.5000 - mse: 1280134.5000 - mae: 323.2657 - val_loss: 1020857.6875 - val_mse: 1020857.5625 - val_mae: 341.7010\n",
      "Epoch 83/15000\n",
      "12943/12943 [==============================] - 10s 780us/step - loss: 1280569.8750 - mse: 1280569.8750 - mae: 322.8354 - val_loss: 1022292.0625 - val_mse: 1022292.0625 - val_mae: 348.2738\n",
      "Epoch 84/15000\n",
      "12943/12943 [==============================] - 10s 786us/step - loss: 1280142.5000 - mse: 1280142.5000 - mae: 323.0081 - val_loss: 1027373.8125 - val_mse: 1027373.8125 - val_mae: 345.7345\n",
      "Epoch 85/15000\n",
      "12943/12943 [==============================] - 10s 789us/step - loss: 1278961.8750 - mse: 1278961.8750 - mae: 322.6590 - val_loss: 1000078.1250 - val_mse: 1000078.1250 - val_mae: 329.1736\n",
      "Epoch 86/15000\n",
      "12943/12943 [==============================] - 10s 786us/step - loss: 1278962.8750 - mse: 1278962.8750 - mae: 322.7601 - val_loss: 1007792.1875 - val_mse: 1007792.1875 - val_mae: 337.2183\n",
      "Epoch 87/15000\n",
      "12943/12943 [==============================] - 10s 780us/step - loss: 1277866.1250 - mse: 1277866.1250 - mae: 322.7798 - val_loss: 1017194.1250 - val_mse: 1017194.1250 - val_mae: 333.3535\n",
      "Epoch 88/15000\n",
      "12943/12943 [==============================] - 10s 787us/step - loss: 1276235.0000 - mse: 1276234.8750 - mae: 322.6111 - val_loss: 1009160.8750 - val_mse: 1009160.8750 - val_mae: 337.8495\n",
      "Epoch 89/15000\n",
      "12943/12943 [==============================] - 10s 807us/step - loss: 1275281.8750 - mse: 1275281.8750 - mae: 322.3434 - val_loss: 1006300.5625 - val_mse: 1006300.5625 - val_mae: 343.7213\n",
      "Epoch 90/15000\n",
      "12943/12943 [==============================] - 10s 772us/step - loss: 1275091.0000 - mse: 1275091.0000 - mae: 322.5636 - val_loss: 1014689.7500 - val_mse: 1014689.8750 - val_mae: 325.0014\n",
      "Epoch 91/15000\n",
      "12943/12943 [==============================] - 10s 791us/step - loss: 1274201.6250 - mse: 1274201.6250 - mae: 322.4721 - val_loss: 1010204.0000 - val_mse: 1010204.0000 - val_mae: 339.7747\n",
      "Epoch 92/15000\n",
      "12943/12943 [==============================] - 10s 796us/step - loss: 1272916.1250 - mse: 1272916.1250 - mae: 322.3139 - val_loss: 1011795.0625 - val_mse: 1011795.0625 - val_mae: 353.2176\n",
      "Epoch 93/15000\n",
      "12943/12943 [==============================] - 10s 795us/step - loss: 1273866.7500 - mse: 1273866.7500 - mae: 322.0436 - val_loss: 1011697.4375 - val_mse: 1011697.4375 - val_mae: 330.6067\n",
      "Epoch 94/15000\n",
      "12943/12943 [==============================] - 10s 784us/step - loss: 1272399.6250 - mse: 1272399.6250 - mae: 322.4773 - val_loss: 1016872.1250 - val_mse: 1016872.1250 - val_mae: 330.0795\n",
      "Epoch 95/15000\n",
      "12943/12943 [==============================] - 10s 783us/step - loss: 1272282.0000 - mse: 1272282.0000 - mae: 322.1605 - val_loss: 1010794.3750 - val_mse: 1010794.3750 - val_mae: 336.0190\n",
      "Epoch 96/15000\n",
      "12943/12943 [==============================] - 10s 791us/step - loss: 1271738.0000 - mse: 1271738.0000 - mae: 322.0129 - val_loss: 1013124.0000 - val_mse: 1013124.0000 - val_mae: 335.4351\n",
      "Epoch 97/15000\n",
      "12943/12943 [==============================] - 10s 789us/step - loss: 1271095.8750 - mse: 1271095.8750 - mae: 321.9479 - val_loss: 1003731.6250 - val_mse: 1003731.6250 - val_mae: 321.6432\n",
      "Epoch 98/15000\n",
      "12943/12943 [==============================] - 10s 798us/step - loss: 1269775.2500 - mse: 1269775.2500 - mae: 322.0810 - val_loss: 1019539.2500 - val_mse: 1019539.2500 - val_mae: 334.7219\n",
      "Epoch 99/15000\n",
      "12943/12943 [==============================] - 10s 778us/step - loss: 1267952.8750 - mse: 1267952.8750 - mae: 321.8432 - val_loss: 1024479.9375 - val_mse: 1024479.9375 - val_mae: 327.0156\n",
      "Epoch 100/15000\n",
      "12943/12943 [==============================] - 10s 777us/step - loss: 1268441.8750 - mse: 1268441.8750 - mae: 321.6024 - val_loss: 1016674.0625 - val_mse: 1016674.0625 - val_mae: 329.1561\n",
      "Epoch 101/15000\n",
      "12943/12943 [==============================] - 10s 772us/step - loss: 1268332.2500 - mse: 1268332.2500 - mae: 321.7212 - val_loss: 1026585.7500 - val_mse: 1026585.7500 - val_mae: 355.5539\n",
      "Epoch 102/15000\n",
      "12943/12943 [==============================] - 10s 801us/step - loss: 1267453.5000 - mse: 1267453.5000 - mae: 321.5990 - val_loss: 1023895.3750 - val_mse: 1023895.3750 - val_mae: 327.9190\n",
      "Epoch 103/15000\n",
      "12943/12943 [==============================] - 10s 781us/step - loss: 1267148.6250 - mse: 1267148.6250 - mae: 321.4749 - val_loss: 1020424.0000 - val_mse: 1020424.0000 - val_mae: 351.8658\n",
      "Epoch 104/15000\n",
      "12943/12943 [==============================] - 10s 787us/step - loss: 1266730.1250 - mse: 1266730.1250 - mae: 321.8293 - val_loss: 1021739.3125 - val_mse: 1021739.3125 - val_mae: 332.2449\n",
      "Epoch 105/15000\n",
      "12943/12943 [==============================] - 10s 779us/step - loss: 1265439.8750 - mse: 1265439.8750 - mae: 321.5302 - val_loss: 1012912.0625 - val_mse: 1012912.0625 - val_mae: 328.9427\n",
      "Epoch 106/15000\n",
      "12943/12943 [==============================] - 10s 801us/step - loss: 1264836.7500 - mse: 1264836.7500 - mae: 321.4384 - val_loss: 1022198.0000 - val_mse: 1022198.0000 - val_mae: 335.4413\n",
      "Epoch 107/15000\n",
      "12943/12943 [==============================] - 10s 794us/step - loss: 1263400.7500 - mse: 1263400.7500 - mae: 321.1895 - val_loss: 1026145.2500 - val_mse: 1026145.2500 - val_mae: 355.5379\n",
      "Epoch 108/15000\n",
      "12943/12943 [==============================] - 10s 783us/step - loss: 1263247.5000 - mse: 1263247.5000 - mae: 321.2939 - val_loss: 1021413.4375 - val_mse: 1021413.4375 - val_mae: 339.1056\n",
      "Epoch 109/15000\n",
      "12943/12943 [==============================] - 10s 777us/step - loss: 1262515.7500 - mse: 1262515.7500 - mae: 321.5452 - val_loss: 1031645.7500 - val_mse: 1031645.7500 - val_mae: 331.6476\n",
      "Epoch 110/15000\n",
      "12943/12943 [==============================] - 10s 789us/step - loss: 1263439.2500 - mse: 1263439.2500 - mae: 321.1848 - val_loss: 1028614.1875 - val_mse: 1028614.1875 - val_mae: 334.1787\n",
      "Epoch 111/15000\n",
      "12943/12943 [==============================] - 10s 784us/step - loss: 1262829.2500 - mse: 1262829.2500 - mae: 321.1018 - val_loss: 1021937.6250 - val_mse: 1021937.6250 - val_mae: 331.6523\n",
      "Epoch 112/15000\n",
      "12943/12943 [==============================] - 10s 792us/step - loss: 1261451.8750 - mse: 1261452.0000 - mae: 321.0063 - val_loss: 1023890.1250 - val_mse: 1023890.1250 - val_mae: 327.4774\n",
      "Epoch 113/15000\n",
      "12943/12943 [==============================] - 10s 806us/step - loss: 1261356.3750 - mse: 1261356.3750 - mae: 321.0369 - val_loss: 1023823.0625 - val_mse: 1023823.0625 - val_mae: 329.7290\n",
      "Epoch 114/15000\n",
      "12943/12943 [==============================] - 10s 782us/step - loss: 1259965.0000 - mse: 1259965.0000 - mae: 321.0314 - val_loss: 1034923.1875 - val_mse: 1034923.1875 - val_mae: 333.7068\n",
      "Epoch 115/15000\n",
      "12943/12943 [==============================] - 10s 781us/step - loss: 1259315.6250 - mse: 1259315.6250 - mae: 321.0113 - val_loss: 1028867.0000 - val_mse: 1028867.0000 - val_mae: 326.2805\n",
      "Epoch 116/15000\n",
      "12943/12943 [==============================] - 10s 778us/step - loss: 1259822.6250 - mse: 1259822.6250 - mae: 320.8242 - val_loss: 1043568.5625 - val_mse: 1043568.5625 - val_mae: 343.5552\n",
      "Epoch 117/15000\n",
      "12943/12943 [==============================] - 10s 794us/step - loss: 1259075.0000 - mse: 1259075.0000 - mae: 321.0889 - val_loss: 1020711.3125 - val_mse: 1020711.3125 - val_mae: 336.9777\n",
      "Epoch 118/15000\n",
      "12943/12943 [==============================] - 11s 825us/step - loss: 1258254.6250 - mse: 1258254.6250 - mae: 320.6994 - val_loss: 1033597.0625 - val_mse: 1033597.0625 - val_mae: 336.2958\n",
      "Epoch 119/15000\n",
      "12943/12943 [==============================] - 10s 806us/step - loss: 1259634.7500 - mse: 1259634.7500 - mae: 320.7358 - val_loss: 1031558.7500 - val_mse: 1031558.7500 - val_mae: 332.0572\n",
      "Epoch 120/15000\n",
      "12943/12943 [==============================] - 10s 789us/step - loss: 1257648.1250 - mse: 1257648.1250 - mae: 320.4710 - val_loss: 1059118.7500 - val_mse: 1059118.7500 - val_mae: 354.4479\n",
      "Epoch 121/15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12943/12943 [==============================] - 10s 786us/step - loss: 1258383.7500 - mse: 1258383.7500 - mae: 320.5467 - val_loss: 1025614.1250 - val_mse: 1025614.1250 - val_mae: 325.4478\n",
      "Epoch 122/15000\n",
      "12943/12943 [==============================] - 10s 776us/step - loss: 1256620.6250 - mse: 1256620.6250 - mae: 320.4567 - val_loss: 1022545.6250 - val_mse: 1022545.6250 - val_mae: 334.6413\n",
      "Epoch 123/15000\n",
      "12943/12943 [==============================] - 10s 782us/step - loss: 1257596.3750 - mse: 1257596.3750 - mae: 320.5582 - val_loss: 1038333.5625 - val_mse: 1038333.5625 - val_mae: 340.6441\n",
      "Epoch 124/15000\n",
      "12943/12943 [==============================] - 10s 793us/step - loss: 1255216.6250 - mse: 1255216.6250 - mae: 320.7180 - val_loss: 1033777.0000 - val_mse: 1033777.0000 - val_mae: 339.9519\n",
      "Epoch 125/15000\n",
      "12943/12943 [==============================] - 10s 791us/step - loss: 1256701.6250 - mse: 1256701.6250 - mae: 320.1794 - val_loss: 1035261.8125 - val_mse: 1035261.8125 - val_mae: 349.4171\n",
      "Epoch 126/15000\n",
      "12943/12943 [==============================] - 10s 785us/step - loss: 1257566.6250 - mse: 1257566.7500 - mae: 320.7719 - val_loss: 1023654.2500 - val_mse: 1023654.2500 - val_mae: 336.5588\n",
      "Epoch 127/15000\n",
      "12943/12943 [==============================] - 10s 777us/step - loss: 1256220.5000 - mse: 1256220.5000 - mae: 320.2726 - val_loss: 1038889.0625 - val_mse: 1038888.9375 - val_mae: 345.6319\n",
      "Epoch 128/15000\n",
      "12943/12943 [==============================] - 10s 779us/step - loss: 1255983.3750 - mse: 1255983.3750 - mae: 320.4210 - val_loss: 1052977.5000 - val_mse: 1052977.5000 - val_mae: 334.7581\n",
      "Epoch 129/15000\n",
      "12943/12943 [==============================] - 10s 776us/step - loss: 1255391.5000 - mse: 1255391.5000 - mae: 320.3981 - val_loss: 1054730.5000 - val_mse: 1054730.5000 - val_mae: 343.6693\n",
      "Epoch 130/15000\n",
      "12943/12943 [==============================] - 10s 786us/step - loss: 1255021.1250 - mse: 1255021.1250 - mae: 319.9272 - val_loss: 1026950.7500 - val_mse: 1026950.7500 - val_mae: 340.8742\n",
      "Epoch 131/15000\n",
      "12943/12943 [==============================] - 10s 782us/step - loss: 1254768.7500 - mse: 1254768.7500 - mae: 320.3723 - val_loss: 1024817.9375 - val_mse: 1024818.1875 - val_mae: 330.8662\n",
      "Epoch 132/15000\n",
      "12943/12943 [==============================] - 10s 785us/step - loss: 1254671.7500 - mse: 1254671.7500 - mae: 320.1598 - val_loss: 1034774.5000 - val_mse: 1034774.5000 - val_mae: 337.4611\n",
      "Epoch 133/15000\n",
      "12943/12943 [==============================] - 10s 777us/step - loss: 1252950.3750 - mse: 1252950.3750 - mae: 320.1894 - val_loss: 1034359.3125 - val_mse: 1034359.3125 - val_mae: 335.8706\n",
      "Epoch 134/15000\n",
      "12943/12943 [==============================] - 10s 775us/step - loss: 1253950.1250 - mse: 1253950.1250 - mae: 319.8731 - val_loss: 1046673.3125 - val_mse: 1046673.3125 - val_mae: 326.6551\n",
      "Epoch 135/15000\n",
      "12943/12943 [==============================] - 10s 775us/step - loss: 1252460.6250 - mse: 1252460.6250 - mae: 319.9249 - val_loss: 1032607.5000 - val_mse: 1032607.5000 - val_mae: 344.6142\n",
      "Epoch 136/15000\n",
      "12943/12943 [==============================] - 10s 771us/step - loss: 1253165.3750 - mse: 1253165.3750 - mae: 319.8123 - val_loss: 1017780.2500 - val_mse: 1017780.2500 - val_mae: 332.9966\n",
      "Epoch 137/15000\n",
      "12943/12943 [==============================] - 10s 786us/step - loss: 1253270.0000 - mse: 1253270.0000 - mae: 320.0108 - val_loss: 1029195.0625 - val_mse: 1029195.0625 - val_mae: 339.8947\n",
      "Epoch 138/15000\n",
      "12943/12943 [==============================] - 10s 790us/step - loss: 1252947.5000 - mse: 1252947.5000 - mae: 319.9391 - val_loss: 1045571.8750 - val_mse: 1045571.8750 - val_mae: 344.0155\n",
      "Epoch 139/15000\n",
      "12943/12943 [==============================] - 10s 775us/step - loss: 1251874.1250 - mse: 1251874.1250 - mae: 320.0682 - val_loss: 1061781.3750 - val_mse: 1061781.3750 - val_mae: 345.7677\n",
      "Epoch 140/15000\n",
      "12943/12943 [==============================] - 10s 788us/step - loss: 1251657.1250 - mse: 1251657.1250 - mae: 319.9145 - val_loss: 1038172.0625 - val_mse: 1038172.0625 - val_mae: 362.2314\n",
      "Epoch 141/15000\n",
      "12943/12943 [==============================] - 10s 773us/step - loss: 1251158.7500 - mse: 1251158.7500 - mae: 320.1746 - val_loss: 1027092.8125 - val_mse: 1027092.8125 - val_mae: 331.8787\n",
      "Epoch 142/15000\n",
      "12943/12943 [==============================] - 10s 777us/step - loss: 1251732.2500 - mse: 1251732.1250 - mae: 319.6949 - val_loss: 1062678.2500 - val_mse: 1062678.2500 - val_mae: 351.3210\n",
      "Epoch 143/15000\n",
      "12943/12943 [==============================] - 10s 789us/step - loss: 1250364.2500 - mse: 1250364.1250 - mae: 320.1513 - val_loss: 1058796.6250 - val_mse: 1058796.6250 - val_mae: 359.6205\n",
      "Epoch 144/15000\n",
      "12943/12943 [==============================] - 10s 789us/step - loss: 1249309.5000 - mse: 1249309.5000 - mae: 319.8637 - val_loss: 1072383.0000 - val_mse: 1072383.0000 - val_mae: 363.2890\n",
      "Epoch 145/15000\n",
      "12943/12943 [==============================] - 10s 779us/step - loss: 1249698.7500 - mse: 1249698.7500 - mae: 319.9918 - val_loss: 1064014.3750 - val_mse: 1064014.3750 - val_mae: 355.0618\n",
      "Epoch 146/15000\n",
      "12943/12943 [==============================] - 10s 781us/step - loss: 1248926.5000 - mse: 1248926.6250 - mae: 319.7555 - val_loss: 1059539.3750 - val_mse: 1059539.3750 - val_mae: 329.3184\n",
      "Epoch 147/15000\n",
      "12943/12943 [==============================] - 10s 794us/step - loss: 1249266.0000 - mse: 1249266.0000 - mae: 320.2159 - val_loss: 1067140.6250 - val_mse: 1067140.6250 - val_mae: 355.4098\n",
      "Epoch 148/15000\n",
      "12943/12943 [==============================] - 10s 790us/step - loss: 1249715.7500 - mse: 1249715.7500 - mae: 320.1262 - val_loss: 1051450.7500 - val_mse: 1051450.8750 - val_mae: 334.4840\n",
      "Epoch 149/15000\n",
      "12943/12943 [==============================] - 10s 770us/step - loss: 1247251.0000 - mse: 1247251.0000 - mae: 319.7798 - val_loss: 1026023.1875 - val_mse: 1026023.1875 - val_mae: 330.9250\n",
      "Epoch 150/15000\n",
      "12943/12943 [==============================] - 10s 779us/step - loss: 1247700.5000 - mse: 1247700.5000 - mae: 319.7538 - val_loss: 1051759.1250 - val_mse: 1051759.1250 - val_mae: 336.2666\n",
      "Epoch 151/15000\n",
      "12943/12943 [==============================] - 10s 783us/step - loss: 1248098.7500 - mse: 1248098.7500 - mae: 319.9780 - val_loss: 1044991.6250 - val_mse: 1044991.6250 - val_mae: 332.3139\n",
      "Epoch 152/15000\n",
      "12943/12943 [==============================] - 10s 780us/step - loss: 1246908.2500 - mse: 1246908.2500 - mae: 319.9478 - val_loss: 1047713.6875 - val_mse: 1047713.6875 - val_mae: 355.9471\n",
      "Epoch 153/15000\n",
      "12943/12943 [==============================] - 10s 786us/step - loss: 1248061.5000 - mse: 1248061.5000 - mae: 320.2094 - val_loss: 1046016.6875 - val_mse: 1046016.6875 - val_mae: 335.6201\n",
      "Epoch 154/15000\n",
      "12943/12943 [==============================] - 10s 793us/step - loss: 1247313.3750 - mse: 1247313.3750 - mae: 320.0419 - val_loss: 1050262.7500 - val_mse: 1050262.7500 - val_mae: 339.5848\n",
      "Epoch 155/15000\n",
      "12943/12943 [==============================] - 10s 777us/step - loss: 1247577.0000 - mse: 1247577.0000 - mae: 319.6545 - val_loss: 1044892.5625 - val_mse: 1044892.5625 - val_mae: 337.7912\n",
      "Epoch 156/15000\n",
      "12943/12943 [==============================] - 10s 803us/step - loss: 1246909.1250 - mse: 1246909.1250 - mae: 320.3558 - val_loss: 1059355.0000 - val_mse: 1059355.0000 - val_mae: 339.9252\n",
      "Epoch 157/15000\n",
      "12943/12943 [==============================] - 10s 791us/step - loss: 1247037.1250 - mse: 1247037.1250 - mae: 319.9602 - val_loss: 1048667.3750 - val_mse: 1048667.3750 - val_mae: 352.6145\n",
      "Epoch 158/15000\n",
      "12943/12943 [==============================] - 10s 791us/step - loss: 1246770.6250 - mse: 1246770.6250 - mae: 319.8563 - val_loss: 1054147.3750 - val_mse: 1054147.3750 - val_mae: 335.4907\n",
      "Epoch 159/15000\n",
      "12943/12943 [==============================] - 10s 788us/step - loss: 1245968.6250 - mse: 1245968.6250 - mae: 319.8220 - val_loss: 1039340.1250 - val_mse: 1039340.0000 - val_mae: 342.8223\n",
      "Epoch 160/15000\n",
      "12943/12943 [==============================] - 10s 783us/step - loss: 1245112.8750 - mse: 1245112.8750 - mae: 319.8093 - val_loss: 1029007.6250 - val_mse: 1029007.6250 - val_mae: 327.9232\n",
      "Epoch 161/15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12943/12943 [==============================] - 10s 783us/step - loss: 1245979.5000 - mse: 1245979.5000 - mae: 319.6443 - val_loss: 1050129.7500 - val_mse: 1050129.7500 - val_mae: 338.4957\n",
      "Epoch 162/15000\n",
      "12943/12943 [==============================] - 10s 790us/step - loss: 1244758.1250 - mse: 1244758.1250 - mae: 319.8222 - val_loss: 1069717.0000 - val_mse: 1069717.0000 - val_mae: 337.9758\n",
      "Epoch 163/15000\n",
      "12943/12943 [==============================] - 10s 782us/step - loss: 1245833.5000 - mse: 1245833.5000 - mae: 319.3469 - val_loss: 1066534.3750 - val_mse: 1066534.3750 - val_mae: 344.3508\n",
      "Epoch 164/15000\n",
      "12943/12943 [==============================] - 10s 774us/step - loss: 1244503.8750 - mse: 1244503.8750 - mae: 319.6278 - val_loss: 1039140.3750 - val_mse: 1039140.3750 - val_mae: 337.3285\n",
      "Epoch 165/15000\n",
      "12943/12943 [==============================] - 10s 791us/step - loss: 1244179.8750 - mse: 1244179.8750 - mae: 319.8733 - val_loss: 1055702.7500 - val_mse: 1055702.7500 - val_mae: 323.9451\n",
      "Epoch 166/15000\n",
      "12943/12943 [==============================] - 10s 778us/step - loss: 1244832.7500 - mse: 1244832.7500 - mae: 319.6378 - val_loss: 1035247.1250 - val_mse: 1035247.1250 - val_mae: 336.1120\n",
      "Epoch 167/15000\n",
      "12943/12943 [==============================] - 10s 784us/step - loss: 1243690.0000 - mse: 1243690.1250 - mae: 319.1304 - val_loss: 1051765.8750 - val_mse: 1051765.8750 - val_mae: 349.0525\n",
      "Epoch 168/15000\n",
      "12943/12943 [==============================] - 10s 768us/step - loss: 1243325.7500 - mse: 1243325.7500 - mae: 319.5183 - val_loss: 1048792.7500 - val_mse: 1048792.7500 - val_mae: 335.3884\n",
      "Epoch 169/15000\n",
      "12943/12943 [==============================] - 10s 784us/step - loss: 1243469.2500 - mse: 1243469.5000 - mae: 319.6203 - val_loss: 1029132.5000 - val_mse: 1029132.5000 - val_mae: 330.1096\n",
      "Epoch 170/15000\n",
      "12943/12943 [==============================] - 10s 769us/step - loss: 1242819.1250 - mse: 1242819.1250 - mae: 319.6284 - val_loss: 1030811.7500 - val_mse: 1030811.7500 - val_mae: 331.4213\n",
      "Epoch 171/15000\n",
      "12943/12943 [==============================] - 10s 791us/step - loss: 1243485.8750 - mse: 1243485.8750 - mae: 319.2325 - val_loss: 1069293.6250 - val_mse: 1069293.6250 - val_mae: 340.1118\n",
      "Epoch 172/15000\n",
      "12943/12943 [==============================] - 10s 796us/step - loss: 1242851.8750 - mse: 1242851.8750 - mae: 319.7768 - val_loss: 1041943.8750 - val_mse: 1041944.0000 - val_mae: 341.5876\n",
      "Epoch 173/15000\n",
      "12943/12943 [==============================] - 10s 774us/step - loss: 1241820.1250 - mse: 1241820.1250 - mae: 319.4189 - val_loss: 1047248.8750 - val_mse: 1047248.8750 - val_mae: 333.1674\n",
      "Epoch 174/15000\n",
      "12943/12943 [==============================] - 10s 781us/step - loss: 1242033.7500 - mse: 1242033.7500 - mae: 319.3669 - val_loss: 1061393.0000 - val_mse: 1061393.0000 - val_mae: 345.2715\n",
      "Epoch 175/15000\n",
      "12943/12943 [==============================] - 10s 788us/step - loss: 1242489.1250 - mse: 1242489.1250 - mae: 319.6863 - val_loss: 1042453.2500 - val_mse: 1042453.2500 - val_mae: 337.2070\n",
      "Epoch 176/15000\n",
      "12943/12943 [==============================] - 10s 787us/step - loss: 1242444.2500 - mse: 1242444.2500 - mae: 319.4916 - val_loss: 1101133.7500 - val_mse: 1101133.7500 - val_mae: 373.2873\n",
      "Epoch 177/15000\n",
      "12943/12943 [==============================] - 10s 796us/step - loss: 1240224.5000 - mse: 1240224.5000 - mae: 319.0988 - val_loss: 1041486.0000 - val_mse: 1041486.0000 - val_mae: 357.9518\n",
      "Epoch 178/15000\n",
      "12943/12943 [==============================] - 10s 778us/step - loss: 1241880.7500 - mse: 1241880.7500 - mae: 319.6989 - val_loss: 1046132.9375 - val_mse: 1046132.9375 - val_mae: 336.0016\n",
      "Epoch 179/15000\n",
      "12943/12943 [==============================] - 10s 771us/step - loss: 1240619.3750 - mse: 1240619.3750 - mae: 319.3957 - val_loss: 1070048.2500 - val_mse: 1070048.2500 - val_mae: 335.4073\n",
      "Epoch 180/15000\n",
      "12943/12943 [==============================] - 10s 794us/step - loss: 1240683.8750 - mse: 1240683.8750 - mae: 319.0440 - val_loss: 1029452.8125 - val_mse: 1029452.8125 - val_mae: 332.1342\n",
      "Epoch 181/15000\n",
      "12943/12943 [==============================] - 10s 794us/step - loss: 1240197.2500 - mse: 1240197.2500 - mae: 319.5890 - val_loss: 1072376.6250 - val_mse: 1072376.6250 - val_mae: 334.3846\n",
      "Epoch 182/15000\n",
      "12943/12943 [==============================] - 10s 774us/step - loss: 1239990.0000 - mse: 1239990.0000 - mae: 319.2112 - val_loss: 1059566.8750 - val_mse: 1059566.8750 - val_mae: 341.9789\n",
      "Epoch 183/15000\n",
      "12943/12943 [==============================] - 10s 787us/step - loss: 1239859.8750 - mse: 1239859.8750 - mae: 319.5897 - val_loss: 1069210.0000 - val_mse: 1069210.0000 - val_mae: 331.3097\n",
      "Epoch 184/15000\n",
      "12943/12943 [==============================] - 10s 772us/step - loss: 1240255.0000 - mse: 1240255.0000 - mae: 319.4273 - val_loss: 1073417.3750 - val_mse: 1073417.3750 - val_mae: 351.5756\n",
      "Epoch 185/15000\n",
      "12943/12943 [==============================] - 10s 787us/step - loss: 1240218.5000 - mse: 1240218.5000 - mae: 319.5188 - val_loss: 1056725.8750 - val_mse: 1056726.0000 - val_mae: 337.0604\n",
      "Epoch 186/15000\n",
      "12943/12943 [==============================] - 10s 787us/step - loss: 1239612.6250 - mse: 1239612.6250 - mae: 319.1746 - val_loss: 1059495.7500 - val_mse: 1059495.7500 - val_mae: 326.5617\n",
      "Epoch 187/15000\n",
      "12943/12943 [==============================] - 10s 781us/step - loss: 1239036.7500 - mse: 1239036.7500 - mae: 319.4026 - val_loss: 1056347.0000 - val_mse: 1056347.0000 - val_mae: 337.8722\n",
      "Epoch 188/15000\n",
      "12943/12943 [==============================] - 10s 787us/step - loss: 1239063.3750 - mse: 1239063.3750 - mae: 319.4740 - val_loss: 1045520.0625 - val_mse: 1045520.0625 - val_mae: 337.6925\n",
      "Epoch 189/15000\n",
      "12943/12943 [==============================] - 10s 779us/step - loss: 1238078.1250 - mse: 1238078.1250 - mae: 319.1402 - val_loss: 1070528.2500 - val_mse: 1070528.2500 - val_mae: 348.1648\n",
      "Epoch 190/15000\n",
      "12943/12943 [==============================] - 10s 779us/step - loss: 1239412.8750 - mse: 1239412.7500 - mae: 319.6507 - val_loss: 1064419.1250 - val_mse: 1064419.2500 - val_mae: 348.5335\n",
      "Epoch 191/15000\n",
      "12943/12943 [==============================] - 10s 779us/step - loss: 1238901.0000 - mse: 1238901.0000 - mae: 319.3017 - val_loss: 1065980.7500 - val_mse: 1065980.7500 - val_mae: 343.7160\n",
      "Epoch 192/15000\n",
      "12943/12943 [==============================] - 10s 783us/step - loss: 1238596.1250 - mse: 1238596.1250 - mae: 319.2298 - val_loss: 1066055.7500 - val_mse: 1066055.7500 - val_mae: 334.6945\n",
      "Epoch 193/15000\n",
      "12943/12943 [==============================] - 10s 783us/step - loss: 1237896.5000 - mse: 1237896.5000 - mae: 319.6488 - val_loss: 1047971.3125 - val_mse: 1047971.3125 - val_mae: 324.7997\n",
      "Epoch 194/15000\n",
      "12943/12943 [==============================] - 10s 772us/step - loss: 1237484.2500 - mse: 1237484.2500 - mae: 319.2858 - val_loss: 1052983.3750 - val_mse: 1052983.3750 - val_mae: 330.4182\n",
      "Epoch 195/15000\n",
      "12943/12943 [==============================] - 11s 815us/step - loss: 1236411.0000 - mse: 1236411.0000 - mae: 319.7205 - val_loss: 1044268.8750 - val_mse: 1044268.8750 - val_mae: 321.1706\n",
      "Epoch 196/15000\n",
      "12943/12943 [==============================] - 10s 785us/step - loss: 1237624.3750 - mse: 1237624.3750 - mae: 319.1921 - val_loss: 1071453.8750 - val_mse: 1071453.8750 - val_mae: 347.0682\n",
      "Epoch 197/15000\n",
      "12943/12943 [==============================] - 10s 784us/step - loss: 1236415.0000 - mse: 1236415.0000 - mae: 319.4358 - val_loss: 1033204.6250 - val_mse: 1033204.6250 - val_mae: 344.9903\n",
      "Epoch 198/15000\n",
      "12943/12943 [==============================] - 10s 771us/step - loss: 1238224.5000 - mse: 1238224.5000 - mae: 319.3938 - val_loss: 1065668.6250 - val_mse: 1065668.7500 - val_mae: 341.4873\n",
      "Epoch 199/15000\n",
      "12943/12943 [==============================] - 10s 785us/step - loss: 1237212.1250 - mse: 1237212.1250 - mae: 319.4214 - val_loss: 1071440.1250 - val_mse: 1071440.1250 - val_mae: 354.6659\n",
      "Epoch 200/15000\n",
      "12943/12943 [==============================] - 10s 789us/step - loss: 1234631.0000 - mse: 1234631.1250 - mae: 319.1670 - val_loss: 1087502.1250 - val_mse: 1087502.1250 - val_mae: 372.5206\n",
      "Epoch 201/15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5834/12943 [============>.................] - ETA: 4s - loss: 1270067.7500 - mse: 1270067.7500 - mae: 321.6425"
     ]
    }
   ],
   "source": [
    "#Execution\n",
    "history = model.fit(x,y, epochs=iteration_step, batch_size=50,  verbose=1, validation_split=0.2)#epochs=max steps batch_size=numer of sample will be trained at once\n",
    "counter=0\n",
    "mse_result=[]\n",
    "while counter<=iteration_step:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)\n",
    "    #Standard OLS performance\n",
    "    from sklearn import linear_model\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit (X_train,y_train)\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    y_pred=reg.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    mse_result.append(mse)\n",
    "    counter=counter+1\n",
    "    #mse = ((y_test-y_pred)**2).mean(axis=ax)\n",
    "k=0\n",
    "for i in mse_result:\n",
    "    k=k+i\n",
    "print(k/len(mse_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290684.0057270916\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i in mse_result:\n",
    "    k=k+i\n",
    "print(k/len(mse_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter=0\n",
    "mse_result=[]\n",
    "while counter<=250:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)\n",
    "    #Standard OLS performance\n",
    "    from sklearn import linear_model\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit (X_train,y_train)\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    y_pred=reg.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    mse_result.append(mse)\n",
    "    counter=counter+1\n",
    "    #mse = ((y_test-y_pred)**2).mean(axis=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1264339.1,\n",
       " 1165512.6,\n",
       " 1193384.8,\n",
       " 1368927.8,\n",
       " 1252641.1,\n",
       " 1205502.8,\n",
       " 1312303.9,\n",
       " 1392657.9,\n",
       " 1325341.8,\n",
       " 1255855.0,\n",
       " 1425882.0,\n",
       " 1311523.9,\n",
       " 1449466.9,\n",
       " 1294085.8,\n",
       " 1314952.5,\n",
       " 1287513.4,\n",
       " 1248532.9,\n",
       " 1177241.1,\n",
       " 1027727.8,\n",
       " 1337614.0,\n",
       " 1335619.6,\n",
       " 1363138.9,\n",
       " 1371232.0,\n",
       " 1239413.6,\n",
       " 1214481.5,\n",
       " 1261798.6,\n",
       " 1245965.5,\n",
       " 1113866.9,\n",
       " 1303190.8,\n",
       " 1398021.2,\n",
       " 1418783.0,\n",
       " 1397415.5,\n",
       " 1225949.9,\n",
       " 1402218.2,\n",
       " 1341407.1,\n",
       " 1277041.4,\n",
       " 1288355.0,\n",
       " 1230199.9,\n",
       " 1148848.4,\n",
       " 1208224.9,\n",
       " 1269293.1,\n",
       " 1490206.9,\n",
       " 1211789.4,\n",
       " 1415818.0,\n",
       " 1338381.9,\n",
       " 1224530.4,\n",
       " 1285846.5,\n",
       " 1333495.8,\n",
       " 1156128.8,\n",
       " 1317940.5,\n",
       " 1119962.5,\n",
       " 1419708.4,\n",
       " 1279503.6,\n",
       " 1221379.9,\n",
       " 1269365.0,\n",
       " 1121339.9,\n",
       " 1220002.8,\n",
       " 1339577.1,\n",
       " 1390597.1,\n",
       " 1252881.6,\n",
       " 1187970.1,\n",
       " 1238791.5,\n",
       " 1377137.2,\n",
       " 1299929.8,\n",
       " 1215083.2,\n",
       " 1199533.5,\n",
       " 1188856.6,\n",
       " 1213820.5,\n",
       " 1347780.0,\n",
       " 1297195.5,\n",
       " 1371952.4,\n",
       " 1304861.6,\n",
       " 1236138.1,\n",
       " 1271544.4,\n",
       " 1282900.9,\n",
       " 1344090.8,\n",
       " 1220098.6,\n",
       " 1408716.6,\n",
       " 1123949.9,\n",
       " 1350606.8,\n",
       " 1207463.8,\n",
       " 1350864.6,\n",
       " 1459634.6,\n",
       " 1162766.5,\n",
       " 1303370.9,\n",
       " 1328494.2,\n",
       " 1226540.2,\n",
       " 1382854.5,\n",
       " 1284502.2,\n",
       " 1304203.1,\n",
       " 1293903.5,\n",
       " 1240619.6,\n",
       " 1271511.9,\n",
       " 1177732.4,\n",
       " 1541738.8,\n",
       " 1353241.0,\n",
       " 1422878.0,\n",
       " 1264608.8,\n",
       " 1142417.6,\n",
       " 1293899.5,\n",
       " 1416917.5,\n",
       " 1292961.2,\n",
       " 1192459.1,\n",
       " 1283038.6,\n",
       " 1397157.5,\n",
       " 1144505.0,\n",
       " 1459453.5,\n",
       " 1279099.0,\n",
       " 1483758.5,\n",
       " 1488756.8,\n",
       " 1169485.4,\n",
       " 1237793.9,\n",
       " 1392996.6,\n",
       " 1375982.6,\n",
       " 1181800.2,\n",
       " 1348725.6,\n",
       " 1221870.1,\n",
       " 1264790.1,\n",
       " 1325998.4,\n",
       " 1185321.2,\n",
       " 1325601.8,\n",
       " 1362168.5,\n",
       " 1184088.9,\n",
       " 1330259.5,\n",
       " 1159802.8,\n",
       " 1311810.8,\n",
       " 1277122.9,\n",
       " 1235319.6,\n",
       " 1370680.8,\n",
       " 1292701.6,\n",
       " 1317722.4,\n",
       " 1345103.5,\n",
       " 1138379.5,\n",
       " 1199730.2,\n",
       " 1372051.4,\n",
       " 1266994.4,\n",
       " 1197125.2,\n",
       " 1128392.1,\n",
       " 1366491.2,\n",
       " 1299165.6,\n",
       " 1274746.8,\n",
       " 1176254.2,\n",
       " 1294569.4,\n",
       " 1194158.9,\n",
       " 1142556.5,\n",
       " 1239646.4,\n",
       " 1313891.8,\n",
       " 1331628.1,\n",
       " 1366803.1,\n",
       " 1345639.5,\n",
       " 1582720.6,\n",
       " 1361237.2,\n",
       " 1168735.1,\n",
       " 1274210.5,\n",
       " 1372023.4,\n",
       " 1525661.4,\n",
       " 1357225.1,\n",
       " 1349540.2,\n",
       " 1221176.1,\n",
       " 1277090.1,\n",
       " 1240882.8,\n",
       " 1295680.4,\n",
       " 1348754.4,\n",
       " 1276207.1,\n",
       " 1251896.6,\n",
       " 1373882.4,\n",
       " 1261470.0,\n",
       " 1279720.4,\n",
       " 1237237.1,\n",
       " 1218648.8,\n",
       " 1269201.5,\n",
       " 1281802.2,\n",
       " 1437059.1,\n",
       " 1152664.9,\n",
       " 1223632.6,\n",
       " 1337514.5,\n",
       " 1296678.8,\n",
       " 1271920.4,\n",
       " 1408770.2,\n",
       " 1290835.2,\n",
       " 1299314.9,\n",
       " 1285652.8,\n",
       " 1233779.0,\n",
       " 1274508.6,\n",
       " 1240935.9,\n",
       " 1294413.1,\n",
       " 1438461.9,\n",
       " 1328452.2,\n",
       " 1244121.8,\n",
       " 1171377.9,\n",
       " 1235554.9,\n",
       " 1384388.6,\n",
       " 1388998.6,\n",
       " 1397250.4,\n",
       " 1264856.6,\n",
       " 1243641.1,\n",
       " 1192758.6,\n",
       " 1102130.6,\n",
       " 1228735.5,\n",
       " 1494089.8,\n",
       " 1313982.0,\n",
       " 1448005.9,\n",
       " 1319941.8,\n",
       " 1280320.6,\n",
       " 1205610.5,\n",
       " 1170123.9,\n",
       " 1234273.1,\n",
       " 1285178.9,\n",
       " 1403209.1,\n",
       " 1226192.5,\n",
       " 1321065.1,\n",
       " 1329648.8,\n",
       " 1245606.0,\n",
       " 1433762.8,\n",
       " 1343866.6,\n",
       " 1347920.1,\n",
       " 1407601.4,\n",
       " 1339254.0,\n",
       " 1350538.8,\n",
       " 1223666.8,\n",
       " 1311751.4,\n",
       " 1251117.5,\n",
       " 1134022.6,\n",
       " 1287719.1,\n",
       " 1256673.8,\n",
       " 1257282.5,\n",
       " 1379706.6,\n",
       " 1234102.8,\n",
       " 1202724.6,\n",
       " 1226907.1,\n",
       " 1383181.6,\n",
       " 1397309.1,\n",
       " 1346845.1,\n",
       " 1334911.6,\n",
       " 1182319.5,\n",
       " 1453054.5,\n",
       " 1242736.5,\n",
       " 1398739.4,\n",
       " 1328954.5,\n",
       " 1194054.0,\n",
       " 1184558.5,\n",
       " 1291956.2,\n",
       " 1414397.9,\n",
       " 1227378.9,\n",
       " 1279687.5,\n",
       " 1184260.0,\n",
       " 1334153.4,\n",
       " 1216244.2,\n",
       " 1348971.6,\n",
       " 1326517.0,\n",
       " 1260566.1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.625100822647362e-05"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor(random_state=5, max_iter=100).fit(X_train, y_train)\n",
    "y_pred=regr.predict(X_test)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
